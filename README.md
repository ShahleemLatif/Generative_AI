# **Project Overview**

This repository contains Python scripts for various AI and machine learning applications, including:

1. **Fine-tuning and Evaluating a GPT-2 Model for Sentiment Classification with PEFT (LoRA)** 
2. **Custom Chatbot with OpenAI Embeddings**  
3. **Image Background Swapping with SAM and Stable Diffusion**  
4. **Real Estate Listing Generation and Personalized Search** 

Each project is designed to be runnable within a Jupyter Notebook environment, with clear steps for setup, execution, and demonstration.

## **1\. Fine-tuning and Evaluating a GPT-2 Model for Sentiment Classification with PEFT (LoRA)**

This project demonstrates the process of loading, evaluating, and fine-tuning a GPT-2 model for sentiment classification using the Parameter-Efficient Fine-Tuning (PEFT) method, specifically LoRA (Low-Rank Adaptation).

### **Key Features**

* **Dataset Loading**: Uses the IMDb dataset for binary sentiment classification (positive/negative).  
* **Tokenization**: Prepares text data using AutoTokenizer for GPT-2, ensuring proper padding and truncation.  
* **Baseline Evaluation**: Establishes a performance baseline for the pre-trained GPT-2 model without fine-tuning.  
* **LoRA Fine-tuning**: Applies LoRA to efficiently fine-tune the GPT-2 model, significantly reducing the number of trainable parameters.  
* **Metrics and Visualization**: Calculates and presents accuracy, precision, recall, F1-score, a classification report, and a confusion matrix.  
* **Model Saving and Loading**: Shows how to save and load the lightweight PEFT adapter for future inference.

### **Technologies Used**

* Python  
* torch  
* tensorflow  
* transformers (for models, tokenizers, and Trainer)  
* datasets  
* peft (for LoRA)  
* numpy  
* scikit-learn (for metrics)  
* matplotlib  
* seaborn  
* os

### **How to Run**

1. Install the required Python packages:  
   pip install transformers peft datasets evaluate accelerate torch scikit-learn matplotlib seaborn  
   pip install \-U git+https://github.com/huggingface/accelerate.git  
   pip install \-U git+https://github.com/huggingface/transformers.git  
   pip install tensorflow  
   pip install \-U datasets huggingface\_hub fsspec  
   pip install peft trl

2. Open and run the Jupyter notebook files in a Jupyter environment.  
3. Execute the cells sequentially to load the dataset, tokenizer, model, perform baseline evaluation, apply LoRA fine-tuning, evaluate the fine-tuned model, and visualize results.

## **2\. Custom Chatbot with OpenAI Embeddings**

This project outlines the development of a custom chatbot that leverages OpenAI's embedding and completion models to answer questions based on a provided knowledge base (e.g., product reviews).

### **Key Features**

* **Data Loading and Processing**: Loads text data from a CSV file and generates OpenAI embeddings for efficient similarity search.  
* **Context Retrieval**: Finds the most relevant pieces of information from the knowledge base based on the user's question using embedding similarity.  
* **Answer Generation**: Uses an OpenAI completion model (gpt-3.5-turbo-instruct by default) to formulate answers based on the retrieved context.  
* **Modular Design**: Implemented as a Chatbot class for reusability and clear separation of concerns.

### **Technologies Used**

* Python  
* pandas  
* tiktoken  
* openai  
* numpy  
* os

### **How to Run**

1. Obtain an OpenAI API key.  
2. Install the required Python packages:  
   pip install pandas tiktoken openai numpy

3. Update the API\_KEY variable in Jupyter notebook files with your actual OpenAI API key.  
4. Prepare your data in a CSV file (e.g., data/books\_rating.csv) with a designated text column (e.g., review/text).  
5. Open and run the Jupyter notebook files in a Jupyter environment.  
6. Execute the cells sequentially to load data, process embeddings, and interact with the chatbot.

## **3\. Image Background Swapping with SAM and Stable Diffusion**

This project demonstrates how to build a web application that allows users to swap the background of a subject in an image with a new background generated by Stable Diffusion based on a text prompt.

### **Key Features**

* **SAM (Segment Anything Model)**: Utilizes Facebook/Meta's pre-trained SAM for precise image segmentation to generate a mask of the subject.  
* **Stable Diffusion Inpainting**: Employs diffusers/stable-diffusion-xl-1.0-inpainting-0.1 for generating new background content based on a text prompt.  
* **Interactive Web App**: Includes a basic web application interface for easy interaction, allowing image uploads and real-time background swapping.

### **Technologies Used**

* Python  
* Pillow (PIL)  
* transformers library (for SAM)  
* diffusers library (for Stable Diffusion)  
* torch  
* numpy

### **How to Run**

1. Ensure you have a GPU available and CUDA configured for optimal performance.  
2. Install the required Python packages:  
   pip install Pillow requests transformers diffusers torch numpy

3. Open and run the Jupyter notebook files in a Jupyter environment.  
4. Follow the instructions in the notebook to load the models, generate masks, perform inpainting, and launch the interactive app.

## **4\. Real Estate Listing Generation and Personalized Search**

This project demonstrates a system that generates diverse real estate listings using a large language model, stores them in a vector database (ChromaDB), and then provides personalized recommendations based on specific buyer preferences.

### **Key Features**

* **Listing Generation**: Uses an LLM (GPT-4o) to generate structured and engaging real estate listings.  
* **Vector Database Integration**: Stores generated listings as documents in ChromaDB with OpenAIEmbeddings for semantic search.  
* **Buyer Preference Collection**: Gathers detailed buyer preferences through an interactive prompt.  
* **Semantic Search**: Utilizes ChromaDB's retriever to find listings that semantically match the buyer's query.  
* **Personalized Descriptions**: Rewrites sections of matching listings to emphasize aspects most relevant to the buyer's stated preferences using an LLM.

### **Technologies Used**

* Python  
* langchain  
* langchain-openai  
* chromadb  
* python-dotenv  
* os

### **How to Run**

1. Obtain an OpenAI API key.  
2. Install the required Python packages:  
   pip install langchain langchain-openai chromadb python-dotenv

3. Create a .env file in the project root and add your OpenAI API key:  
   OPEN\_API\_KEY="YOUR\_OPENAI\_API\_KEY"

   Alternatively, directly replace os.getenv("OPEN\_API\_KEY") with your key in Untitled-4.py.  
4. Open and run the Untitled-4.py notebook in a Jupyter environment.  
5. Execute the cells sequentially. The script will first generate listings, store them, and then prompt you for buyer preferences to demonstrate the personalized search and description generation.
