{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb3be2a-f1d7-4188-9b1c-7a06faab06a9",
   "metadata": {},
   "source": [
    "# Custom Chatbot - Jupyter Notebook Structure\n",
    "\n",
    "### This notebook demonstrates a structured approach to building a custom chatbot using OpenAI's embeddings and completion models, designed for interactive execution within a Jupyter environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc80681-d87e-4c23-a971-ceb54e7b9c5e",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "### First, we'll import all necessary libraries. It's good practice to put all imports at the top of your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11881c6e-dc78-422f-8df8-36776b4dd110",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai.embeddings_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_embedding, distances_from_embeddings\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai.embeddings_utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, distances_from_embeddings\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01529774-ac41-4fd2-abfc-fd682991fcb8",
   "metadata": {},
   "source": [
    "## Chatbot Class Definition\n",
    "### We define the `Chatbot` class, encapsulating all the core logic for data loading, embedding generation, context retrieval, and answer generation. \n",
    "### This makes the code modular and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f21bd-a4e3-4138-adcf-79f6489c4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    \"\"\"\n",
    "    A class to build and run a custom chatbot using OpenAI's embeddings and completion models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_base: str, api_key: str, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initializes the Chatbot with OpenAI API credentials and models.\n",
    "\n",
    "        Args:\n",
    "            api_base (str): The base URL for the OpenAI API.\n",
    "            api_key (str): The API key for OpenAI.\n",
    "            embedding_model (str): The name of the embedding model to use.\n",
    "        \"\"\"\n",
    "        openai.api_base = api_base\n",
    "        openai.api_key = api_key\n",
    "        self.embedding_model = embedding_model\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.data_df = None # DataFrame to store text and embeddings\n",
    "        self.prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the question can't be answered, say \"I don't know\"\n",
    "\n",
    "Context:\n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\n",
    "\"\"\"\n",
    "        self.max_token_count = 2000\n",
    "\n",
    "    def load_and_process_data(self, file_path: str, text_column: str, num_rows: int = None):\n",
    "        \"\"\"\n",
    "        Loads data from a CSV file, extracts text, and generates embeddings.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the CSV file.\n",
    "            text_column (str): The name of the column containing the text reviews.\n",
    "            num_rows (int, optional): Number of rows to read from the CSV. Defaults to None (all rows).\n",
    "\n",
    "        Returns:\n",
    "            bool: True if data was loaded and processed successfully, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Error: File not found at {file_path}\")\n",
    "                return False\n",
    "\n",
    "            df = pd.read_csv(file_path, nrows=num_rows)\n",
    "            if text_column not in df.columns:\n",
    "                print(f\"Error: Text column '{text_column}' not found in the DataFrame.\")\n",
    "                return False\n",
    "\n",
    "            self.data_df = pd.DataFrame(df[text_column].copy())\n",
    "            self.data_df.rename(columns={text_column: \"text\"}, inplace=True)\n",
    "\n",
    "            print(f\"Generating embeddings for {len(self.data_df)} texts...\")\n",
    "            response = openai.Embedding.create(\n",
    "                input=self.data_df[\"text\"].tolist(),\n",
    "                model=self.embedding_model\n",
    "            )\n",
    "            self.data_df['embeddings'] = [data[\"embedding\"] for data in response[\"data\"]]\n",
    "            print(\"Embeddings generated successfully.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_relevant_context(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Finds the most relevant context from the loaded data based on the question's embedding.\n",
    "\n",
    "        Args:\n",
    "            question (str): The user's question.\n",
    "\n",
    "        Returns:\n",
    "            str: A string containing the most relevant context, joined by '###'.\n",
    "        \"\"\"\n",
    "        if self.data_df is None or self.data_df.empty:\n",
    "            print(\"Error: No data loaded. Please call load_and_process_data first.\")\n",
    "            return \"\"\n",
    "\n",
    "        try:\n",
    "            question_embedding = get_embedding(question, engine=self.embedding_model)\n",
    "            self.data_df[\"distances\"] = distances_from_embeddings(\n",
    "                question_embedding, self.data_df['embeddings'].tolist(), distance_metric=\"cosine\"\n",
    "            )\n",
    "            sorted_df = self.data_df.sort_values(by=[\"distances\"], ascending=True)\n",
    "\n",
    "            current_token_count = len(self.tokenizer.encode(self.prompt_template)) + \\\n",
    "                                  len(self.tokenizer.encode(question))\n",
    "\n",
    "            context_texts = []\n",
    "            for text in sorted_df[\"text\"].values:\n",
    "                text_token_count = len(self.tokenizer.encode(text))\n",
    "                if current_token_count + text_token_count <= self.max_token_count:\n",
    "                    context_texts.append(text)\n",
    "                    current_token_count += text_token_count\n",
    "                else:\n",
    "                    break\n",
    "            return \"\\n\\n###\\n\\n\".join(context_texts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting relevant context: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def generate_answer(self, question: str, context: str, model: str = \"gpt-3.5-turbo-instruct\") -> str:\n",
    "        \"\"\"\n",
    "        Generates an answer to the question using the OpenAI completion model and provided context.\n",
    "\n",
    "        Args:\n",
    "            question (str): The user's question.\n",
    "            context (str): The relevant context retrieved for the question.\n",
    "            model (str): The OpenAI completion model to use.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated answer.\n",
    "        \"\"\"\n",
    "        if not context:\n",
    "            return \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        full_prompt = self.prompt_template.format(context, question)\n",
    "        try:\n",
    "            response = openai.Completion.create(model=model, prompt=full_prompt)\n",
    "            return response[\"choices\"][0][\"text\"].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating answer: {e}\")\n",
    "            return \"I encountered an error while trying to answer your question.\"\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        High-level method to ask a question to the chatbot.\n",
    "\n",
    "        Args:\n",
    "            question (str): The user's question.\n",
    "\n",
    "        Returns:\n",
    "            str: The chatbot's answer.\n",
    "        \"\"\"\n",
    "        print(f\"\\nUser Question: {question}\")\n",
    "        context = self.get_relevant_context(question)\n",
    "        if not context:\n",
    "            return \"I couldn't find any relevant information in my knowledge base.\"\n",
    "        answer = self.generate_answer(question, context)\n",
    "        print(f\"Chatbot Answer: {answer}\")\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fafb8d-ae2c-43c3-9c70-6735ff9a6430",
   "metadata": {},
   "source": [
    "## Configuration and Initialization\n",
    "### Here, we define our API keys and data file paths. It's crucial to replace \"YOUR_OPENAI_API_KEY\" with your actual key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc4e7c-4143-4972-b076-5b7451973302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "API_BASE = \"https://openai.vocareum.com/v1\"\n",
    "API_KEY = \"voc-###################\" # Replace with your actual API key or environment variable\n",
    "DATA_FILE = \"data/books_rating.csv\"\n",
    "TEXT_COLUMN = \"review/text\"\n",
    "NUM_ROWS_TO_LOAD = 100 # For demonstration, load only 100 rows\n",
    "\n",
    "# Initialize the chatbot\n",
    "chatbot = Chatbot(api_base=API_BASE, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf7eda-c51c-48e3-93c5-9c0e0197a8e0",
   "metadata": {},
   "source": [
    "### Load and Process Data\n",
    "\n",
    "### This cell will load your data and generate embeddings. This step might take some time depending on the size of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f649cee-cfa3-4d14-a82b-49b0e64244aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "if chatbot.load_and_process_data(DATA_FILE, TEXT_COLUMN, NUM_ROWS_TO_LOAD):\n",
    "    print(\"\\nChatbot is ready to answer questions!\")\n",
    "else:\n",
    "    print(\"Chatbot could not be initialized due to data loading issues. Please check the file path and column name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55956708-665a-48f6-80da-c7e70eaa3f1b",
   "metadata": {},
   "source": [
    "## Ask Questions\n",
    "### Now you can interact with the chatbot by asking questions. Each chatbot.ask() call will retrieve relevant context and generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9390dc04-4dc5-4b71-8be5-0fba8cef1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Questions\n",
    "print(\"\\n--- Asking Questions ---\")\n",
    "chatbot.ask(\"Was the review positive or negative?\")\n",
    "chatbot.ask(\"How many reviews are regarding Dr. Seuss?\")\n",
    "chatbot.ask(\"What is the general sentiment about the books?\")\n",
    "chatbot.ask(\"Tell me about the most common themes in the reviews.\")\n",
    "chatbot.ask(\"What is the average rating of the books?\") # This might lead to \"I don't know\" if context doesn't contain ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17580d52-ae01-4d9c-b029-b334f7ea7de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b47dfd2-1429-48a0-af38-c5fb025a8d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347364c-af29-48c4-81bb-ec16f4a997ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1682535-f1fe-4f9f-9075-46c500d03f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce44f0d2-180e-4da5-aff7-56ae9c4df4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
